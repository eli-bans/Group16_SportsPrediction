# -*- coding: utf-8 -*-
"""16_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G9JK-WBxKpxgCFY9MdMdXDTwcp5teZ0K
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer as si
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from scipy.stats import norm

"""**Import datasets for training the model and testing the model with new data**"""

players_21 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/players_21.csv') # create and train model
players_22 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/players_22.csv') # test model

players_21.head()

players_21.info(verbose=True, show_counts=True)

"""## **Preprocessing the data**

Remove the clearly useless variables
"""

# take out url and id values because they are categorised as useless
useless = ['player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url', 'sofifa_id', 'player_url']
players_21.drop(columns=useless, inplace=True)
players_21

"""Remove attributes with 30% or more missing values"""

null_attributes = players_21.columns[players_21.count()/18944 < 0.7]
players_21.drop(columns=null_attributes, inplace=True)

players_21.shape

"""Convert positional ratings to numeric values since they are are ratings of how well players play at these specific positions"""

other_positions = ['gk', 'rb', 'rcb', 'cb', 'lcb', 'lb', 'rwb', 'rdm', 'cdm', 'ldm', 'lwb', 'rm', 'rcm', 'cm', 'lcm', 'lm', 'ram', 'cam', 'lam', 'rw', 'rf', 'cf', 'lf', 'lw', 'rs', 'st', 'ls']
positions = players_21[other_positions]
positions

"""create a function to remove '+' signs and following values from the position values"""

def remove_additions(cell):
  if len(cell) > 2:
    return cell[:2]
  else:
    return cell

"""Apply the function to the cells of the positions"""

new_positions = positions.applymap(remove_additions)
new_positions

"""Create a function to properly convert these sliced values to integer values"""

def safe_convert(value):
    try:
        return int(value)
    except (ValueError, TypeError):
        return np.nan

new_positions = new_positions.applymap(safe_convert)
new_positions

"""Remove the position attributes from the players_21 data frame and replace them with the new maniputated position values"""

players_21.drop(columns=other_positions, inplace=True)

players_21 = pd.concat([players_21, new_positions], axis=1)
players_21

"""Extract and Show all categorical attributes based on the data types"""

categorical_df = players_21.select_dtypes(include=['object'])
categorical_df

"""Select and show only the attributes that are numerical attributes"""

numerical_df = players_21.select_dtypes(exclude=['object'])
numerical_df

# Show to check which attributes have null values in the categorical dataset
categorical_df.info(verbose=True, show_counts=True)

"""Select the attributes with null values based on the count of non-null rows and impute them using a forward fill"""

# Select categorical attributes with null values
columns_to_fill = categorical_df.columns[categorical_df.count() < 18944]
columns_to_fill

# Forward fill attributes with na values
categorical_df[columns_to_fill] = categorical_df[columns_to_fill].fillna(method='ffill')

# Check to see that there are no null values
categorical_df.info(verbose=True, show_counts=True)

"""Create an object to encode the categorical variables and fit_transform all the cells inside of the data frame that contains just categorical variables"""

# Object to encode categorical values
encoder = LabelEncoder()

# label encode categorical values
categorical_df= categorical_df.apply(encoder.fit_transform)

categorical_df

"""Create object and impute the data frame containing numerical attributes to get rid of null values"""

# Impute numerical attributes
imputer = si(strategy='median')

# checking if null values exist in numerical attributes
numerical_df.info(verbose=True, show_counts=True)

imputed_numerical = imputer.fit_transform(numerical_df)
imputed_numerical_df = pd.DataFrame(imputed_numerical, columns=numerical_df.columns)
imputed_numerical_df

"""Combine both the dataframes of the the categorical and numerical attributes into a single data frame"""

new_players_21 = pd.concat([imputed_numerical_df, categorical_df], axis=1)
new_players_21

# Confirm that there are no null values
new_players_21.info(verbose=True, show_counts=True)

# change naming of variable due to preference
players_21 = new_players_21

"""## Feature Engineering

**Select the needed attributes to train the regression model with**

Here, I picked attributes with a correlation of more than |0.5| to ensure that we get the attributes with a strong correlation.
"""

correlation_vals = players_21.corr()['overall'] # check correlation of other attributes with overall
corr_attributes = correlation_vals[abs(correlation_vals.values) >= 0.5]  # which attributes have a strong correlation with the overall rating
corr_attributes.index

"""**Create new dataframe of needed attributes**"""

needed_att_players_21 = players_21[corr_attributes.index]
needed_att_players_21

"""**Scale dataset inputs only, by separating output attribute from the rest of the dataset**

This is to prevent overall score from being scaled. It was separated from the rest of the dataset and stored in a separate variable (overall_att) before going ahead to scale the input attributes
"""

overall_att = needed_att_players_21.overall

model_data_players_21 = needed_att_players_21.drop(columns='overall')
model_data_players_21

"""Create Standard Scaler object (sc)"""

sc = StandardScaler()

# scale input values of data
scaled_data = sc.fit_transform(model_data_players_21)
ready_data_players_21 = pd.DataFrame(scaled_data, columns=model_data_players_21.columns)
ready_data_players_21

"""## Model development and testing

**Using the test/train split and splitting the X(input attributes) and Y(Output attribute) to train and test the models**
"""

X = ready_data_players_21
Y = overall_att

# test/train split for cross-validation
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)

Xtrain.shape

"""**Train, test, and measure accuracy of RandomForestRegressor model with training split of dataset and testing split of dataset**

Create a RandomForestRegressor model (rf_model).
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error
"""

# Instantiate Regression model
rf_model = RandomForestRegressor(n_estimators = 50, random_state=42)

# train the regression model
rf_model.fit(Xtrain, Ytrain)

# use model for prediction
model_prediction = rf_model.predict(Xtest)

# Measure accuracy
mean_abs_err_results = mean_absolute_error(Ytest, model_prediction)
mean_abs_err_results

"""**Train, test, and measure accuracy of Gradient Boosting model with training split of dataset and testing split of dataset**

Create  and train a GradientBoosting model (gb_model)
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error
"""

# Instantiate GradientBoosting model
gb_model = GradientBoostingRegressor(n_estimators=50, random_state=42)

# Train the GradientBoosting model
gb_model.fit(Xtrain, Ytrain)

# use GradientBoosting model for prediction
gb_prediction = gb_model.predict(Xtest)

gb_mean_abs_err = mean_absolute_error(Ytest, gb_prediction)
gb_mean_abs_err

"""**Train, test, and measure accuracy of XGboost model with training split of dataset and testing split of daaset**

Create  and train a XGboost model (xg_model).
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error.
"""

# Create model that looks to minimize mean squared error as much as possible
xg_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=50)

# train model
xg_model.fit(Xtrain, Ytrain)

# predict with model
xg_prediction = xg_model.predict(Xtest)

xg_mean_abs_err = mean_absolute_error(Ytest, xg_prediction)
xg_mean_abs_err

"""**Train, test and measure accuracy of VotingRegressor ensemble using all previously used models**

VotingRegressor is used as an ensemble model and the voting method is soft because it gives better results.
RandomForest is given a weight of 0.8 because it consistently performs better than the others. The Gradient Boosting ('gradientboosing') and XGBoost ('xgb') models are given lower weights of 0.1 each, suggesting that their contributions are considered less significant relative to the Random Forest. These weights were assigned based on empirical experimentation and observing how different weight combinations impact the overall ensemble performance.
"""

# Create model instance
soft_ensemble = VotingRegressor(estimators=[
    ('randomforest', rf_model),
    ('gradientboosing', gb_model),
    ('xgb', xg_model)
], weights=[0.8, 0.1, 0.1])

soft_ensemble.fit(Xtrain, Ytrain)

ensemble_prediction = soft_ensemble.predict(Xtest)

"""Check accuracy of the VotingRegressor model using mean absolute error"""

ensemble_mean_abs_err = mean_absolute_error(Ytest, ensemble_prediction)
ensemble_mean_abs_err

"""**Compare the different absolute error values of the different models used and select the model to go with**"""

print("RandomForest Model: ", mean_abs_err_results)
print("GradientBoosting Model: ", gb_mean_abs_err)
print("XGBBoost Model: ", xg_mean_abs_err)
print("SoftEnsemble Model: ", ensemble_mean_abs_err)

"""**From observation, the RandomForestRegressor Model is the more accurate one so it's the preference to fine tune**

Fine tune the randomforestregressor model using the RandomizedSearchCV to find the weights needed to give the highest level of accuracy (best score).
The parameter grid for the RandomizedSearchCV, specifies that only the 'n_estimators' parameter is being tuned. The negative mean absolute error is used because RandomizedSearchCV seeks to maximize the scoring metric, and we want to minimize mean absolute error.

## Fine Tuning and hyperparameterisation
"""

# Fine tune hyperparameters of model
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]
max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
min_samples_split = [2, 5, 10, 15, 20]
param_grid = {'n_estimators':n_estimators, 'max_depth':max_depth, 'min_samples_split':min_samples_split}

random_search = RandomizedSearchCV(estimator=rf_model,
                                   param_distributions=param_grid,
                                   n_iter=10, random_state=7,
                                   scoring= 'neg_mean_absolute_error')

"""Find the best value for the mean absolute error and the parameter value (n_estimators) needed to provide this best score"""

random_search.fit(X, Y)

print('best estimator: ', random_search.best_estimator_)
print('best param: ', random_search.best_params_)

"""**Implement hyper parameter after fine tuning.**

Create a RandomForestRegressor model with 1155 decision trees, a max_depth of 40, and set the random seed to 42 for reproducibility.
"""

rf_model_2 = RandomForestRegressor(n_estimators = 1155, min_samples_split=2, max_depth=40, random_state=42)
rf_model_2.fit(Xtrain, Ytrain)
model_prediction_2 = rf_model_2.predict(Xtest)

"""Check accuracy of model using mean absolute error"""

mean_abs_err_results_2 = mean_absolute_error(Ytest, model_prediction_2)
mean_abs_err_results_2

"""## Testing model on New data

**Prepare players_22 dataset for testing model on new unseen data to check for overfitting**

Pick out only the needed attributes from the dataset
"""

# Prepping players_22 to use for testing
df_22 = players_22[corr_attributes.index]
df_22

"""**Separate rcm, lcm, and cm to convert to integer values**"""

positions_to_change = df_22[['rcm', 'cm', 'lcm']]
positions_to_change

df_22.drop(columns=['rcm', 'cm', 'lcm'], inplace=True)
df_22

"""convert the position values into integers just like was done for the players_21 dataset. The same functions were used"""

positions_to_change = positions_to_change.applymap(remove_additions).applymap(safe_convert)
positions_to_change

# Checking if null values exist
positions_to_change.info(verbose=True, show_counts=True)

df_22 = pd.concat([df_22, positions_to_change], axis=1)
df_22

"""Check if null values exist"""

df_22.info()

"""Remove the null values by imputing the values in the dataset"""

imputed_df_22 = imputer.fit_transform(df_22)
imputed_df_22 = pd.DataFrame(imputed_df_22, columns=df_22.columns)
imputed_df_22

imputed_df_22.info()

Y = imputed_df_22.overall
X = imputed_df_22.drop(columns='overall')

X_scaled = sc.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)
X

"""**Use model to predict with new data and check accuracy**"""

# Model on new players_22 data
players_22_prediction = rf_model_2.predict(X)
mean_absolute_error(Y, players_22_prediction)

"""## Save Model

**Saving Model and Scaler object for deployment purposes**
"""

import pickle

"""Both the scaler and model objects are saved in files. Scaler objects are used to ensure that the input values of users in the deployed site are scaled using the same standard deviation values as the one with which the model was trained with."""

# Scaler object as file to scale user input with the necessary parameters
pickle.dump(sc, open('/content/drive/MyDrive/Colab Notebooks/scaler_parameters.pkl', 'wb'))

# Model object as file to predict the scaled values after user input
pickle.dump(rf_model_2, open('/content/drive/MyDrive/Colab Notebooks/player_ratings_model.pkl', 'wb'))