# -*- coding: utf-8 -*-
"""16_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ETcCNZLpvpYPF9OwsNUtwsLMc61hDpvv
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer as si
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, AdaBoostRegressor
from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from scipy.stats import norm

"""**Import datasets for training the model and testing the model with new data**"""

players_21 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/players_21.csv') # create and train model
players_22 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/players_22.csv') # test model

players_21.head()

players_21.info(verbose=True, show_counts=True)

"""## **Preprocessing the data**

Remove the clearly useless variables according to the description of the attributes in the dataset. These attributes are not particularly usefull to the rating of a player because they are either links or they describe how players perform in other positions
"""

# take out url and id values because they are categorised as useless
useless = ['player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url', 'sofifa_id', 'player_url','gk', 'rb', 'rcb', 'cb', 'lcb', 'lb', 'rwb', 'rdm', 'cdm', 'ldm', 'lwb', 'rm', 'rcm', 'cm', 'lcm', 'lm', 'ram', 'cam', 'lam', 'rw', 'rf', 'cf', 'lf', 'lw', 'rs', 'st', 'ls']
players_21.drop(columns=useless, inplace=True)
players_21

"""Remove attributes with 30% or more missing values"""

null_attributes = players_21.columns[players_21.count()/18944 < 0.7]
players_21.drop(columns=null_attributes, inplace=True)

players_21.shape

players_21

"""Extract and Show all categorical attributes based on the data types"""

categorical_df = players_21.select_dtypes(include=['object'])
categorical_df

"""Select and show only the attributes that are numerical attributes"""

numerical_df = players_21.select_dtypes(exclude=['object'])
numerical_df

# Show to check which attributes have null values in the categorical dataset
categorical_df.info(verbose=True, show_counts=True)

"""Select the attributes with null values based on the count of non-null rows and impute them using a forward fill"""

# Select categorical attributes with null values
columns_to_fill = categorical_df.columns[categorical_df.count() < 18944]
columns_to_fill

# Forward fill attributes with na values
categorical_df[columns_to_fill] = categorical_df[columns_to_fill].fillna(method='ffill')

# Check to see that there are no null values
categorical_df.info(verbose=True, show_counts=True)

"""Create an object to encode the categorical variables and fit_transform all the cells inside of the data frame that contains just categorical variables"""

# Object to encode categorical values
encoder = LabelEncoder()

# label encode categorical values
categorical_df= categorical_df.apply(encoder.fit_transform)

categorical_df

"""Create object and impute the data frame containing numerical attributes to get rid of null values"""

# Impute numerical attributes
imputer = si(strategy='median')

# checking if null values exist in numerical attributes
numerical_df.info(verbose=True, show_counts=True)

imputed_numerical = imputer.fit_transform(numerical_df)
imputed_numerical_df = pd.DataFrame(imputed_numerical, columns=numerical_df.columns)
imputed_numerical_df

"""Combine both the dataframes of the the categorical and numerical attributes into a single data frame"""

new_players_21 = pd.concat([imputed_numerical_df, categorical_df], axis=1)
new_players_21

# Confirm that there are no null values
new_players_21.info(verbose=True, show_counts=True)

# change naming of variable due to preference
players_21 = new_players_21

"""## Feature Engineering

**Select the needed attributes to train the regression model with**

Here, I picked attributes with a correlation of more than |0.5| to ensure that we get the attributes with a strong correlation.
"""

correlation_vals = players_21.corr()['overall'] # check correlation of other attributes with overall
corr_attributes = correlation_vals[abs(correlation_vals.values) >= 0.5]  # which attributes have a strong correlation with the overall rating
corr_attributes.index

"""**Create new dataframe of needed attributes**"""

needed_att_players_21 = players_21[corr_attributes.index]
needed_att_players_21

"""**Scale dataset inputs only, by separating output attribute from the rest of the dataset**

This is to prevent overall score from being scaled. It was separated from the rest of the dataset and stored in a separate variable (overall_att) before going ahead to scale the input attributes
"""

overall_att = needed_att_players_21.overall

model_data_players_21 = needed_att_players_21.drop(columns='overall')
model_data_players_21

"""Create Standard Scaler object (sc)"""

sc = StandardScaler()

# scale input values of data
scaled_data = sc.fit_transform(model_data_players_21)
ready_data_players_21 = pd.DataFrame(scaled_data, columns=model_data_players_21.columns)
ready_data_players_21

"""## Model development and testing

**Using the test/train split and splitting the X(input attributes) and Y(Output attribute) to train and test the models**
"""

X = ready_data_players_21
Y = overall_att

# test/train split for cross-validation
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)

Xtrain.shape

"""**Train, test, and measure accuracy of RandomForestRegressor model with training split of dataset and testing split of dataset**

Create a RandomForestRegressor model (rf_model).
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error
"""

# Instantiate Regression model
rf_model = RandomForestRegressor(n_estimators=2000, max_depth=1000, n_jobs=-1, random_state=42)

# train the regression model
rf_model.fit(Xtrain, Ytrain)

# use model for prediction
model_prediction = rf_model.predict(Xtest)

# Measure accuracy
mean_abs_err_results = mean_absolute_error(Ytest, model_prediction)
mean_abs_err_results

"""**Train, test and measure accuracy of AdaBoostRegressor model with training split of dataset and testing split of dataset**

Create an AdaBoostRegressor with a base model as a decisiontreeregressor
"""

base_model = DecisionTreeRegressor(max_depth=1000)

adaboost_regressor = AdaBoostRegressor(base_model, n_estimators=1000, random_state=42)

adaboost_regressor.fit(Xtrain, Ytrain)

ada_predict = adaboost_regressor.predict(Xtest)
mae = mean_absolute_error(Ytest, ada_predict)
mae

"""**Train, test, and measure accuracy of Gradient Boosting model with training split of dataset and testing split of dataset**

Create  and train a GradientBoosting model (gb_model)
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error
"""

# Instantiate GradientBoosting model
gb_model = GradientBoostingRegressor(n_estimators=50, random_state=42)

# Train the GradientBoosting model
gb_model.fit(Xtrain, Ytrain)

# use GradientBoosting model for prediction
gb_prediction = gb_model.predict(Xtest)

gb_mean_abs_err = mean_absolute_error(Ytest, gb_prediction)
gb_mean_abs_err

"""**Train, test, and measure accuracy of XGboost model with training split of dataset and testing split of daaset**

Create  and train a XGboost model (xg_model).
Use the test data-set ( Xtest ) to make predictions and then check the accuracy using mean absolute error.
"""

# Create model that looks to minimize mean squared error as much as possible
xg_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=50)

# train model
xg_model.fit(Xtrain, Ytrain)

# predict with model
xg_prediction = xg_model.predict(Xtest)

xg_mean_abs_err = mean_absolute_error(Ytest, xg_prediction)
xg_mean_abs_err

"""**Train, test and measure accuracy of VotingRegressor ensemble using all previously used models**

VotingRegressor is used as an ensemble model and the voting method is soft because it gives better results.
RandomForest is given a weight of 0.8 because it consistently performs better than the others. The Gradient Boosting ('gradientboosing') and XGBoost ('xgb') models are given lower weights of 0.1 each, suggesting that their contributions are considered less significant relative to the Random Forest. These weights were assigned based on empirical experimentation and observing how different weight combinations impact the overall ensemble performance.
"""

# Create model instance
soft_ensemble = VotingRegressor(estimators=[
    ('randomforest', rf_model),
    ('gradientboosing', gb_model),
    ('xgb', xg_model)
], weights=[0.8, 0.1, 0.1])

soft_ensemble.fit(Xtrain, Ytrain)

ensemble_prediction = soft_ensemble.predict(Xtest)

"""Check accuracy of the VotingRegressor model using mean absolute error"""

ensemble_mean_abs_err = mean_absolute_error(Ytest, ensemble_prediction)
ensemble_mean_abs_err

"""**Compare the different absolute error values of the different models used and select the model to go with**"""

print("RandomForest Model: ", mean_abs_err_results)
print("RandomForest Model: ", mae)
print("GradientBoosting Model: ", gb_mean_abs_err)
print("XGBBoost Model: ", xg_mean_abs_err)
print("SoftEnsemble Model: ", ensemble_mean_abs_err)

"""**From observation, the AdaBoostRegressor Model is the more accurate one so it's the preference to fine tune**

Fine tune the AdaBoostRegressor model using the GridSearchCV to find the weights needed to give the highest level of accuracy (best score).
The parameter grid for the GridSearchCv, specifies that the 'n_estimators' parameter is being tuned along with the max_depth, and learning_rate. The negative mean absolute error is used because GridSearchCv seeks to maximize the scoring metric, and we want to minimize mean absolute error.

## Fine Tuning and hyperparameterisation
"""

# Fine tune hyperparameters of model
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]
max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
min_samples_split = [2, 5, 10, 15, 20]
param_grid = {'n_estimators':n_estimators, 'max_depth':max_depth, 'min_samples_split':min_samples_split}

cv=KFold(n_splits=3)
# random_search = RandomizedSearchCV(estimator=rf_model,
#                                    param_distributions=param_grid,
#                                    n_iter=10, random_state=7, cv=cv,
#                                    scoring= 'neg_mean_absolute_error')

random_search = GridSearchCV(rf_model,param_grid=param_grid,cv=cv,scoring="neg_mean_absolute_error")

cv=KFold(n_splits=3)
n_estimators = [1000, 2000, 4000]
learning_rate = [0.1, 0.2, 0.3]
param_grid = {'n_estimators':n_estimators,'learning_rate':learning_rate}

random_search = GridSearchCV(adaboost_regressor,param_grid=param_grid,cv=cv,scoring="neg_mean_absolute_error", n_jobs=3)

"""Find the best value for the mean absolute error and the parameter value (n_estimators) needed to provide this best score"""

random_search.fit(X, Y)

print('best estimator: ', random_search.best_estimator_)
print('best param: ', random_search.best_params_)

"""**Implement hyper parameter after fine tuning.**

Create a AdaBoostRegressor model with 1000 depth decision tree model, and 2000 as the number of estimators
"""

base_model_2 = DecisionTreeRegressor(max_depth=1000)
model_2 = AdaBoostRegressor(base_model_2, n_estimators=2000, learning_rate=0.2, random_state=42)
model_2.fit(Xtrain, Ytrain)

model_prediction_2 = model_2.predict(Xtest)

"""Check accuracy of model using mean absolute error"""

mean_abs_err_results_2 = mean_absolute_error(Ytest, model_prediction_2)
mean_abs_err_results_2

"""## Testing model on New data

**Prepare players_22 dataset for testing model on new unseen data to check for overfitting**

Pick out only the needed attributes from the dataset
"""

# Prepping players_22 to use for testing
df_22 = players_22[corr_attributes.index]
df_22

"""Check if null values exist"""

df_22.info()

"""Remove the null values by imputing the values in the dataset"""

imputed_df_22 = imputer.fit_transform(df_22)
imputed_df_22 = pd.DataFrame(imputed_df_22, columns=df_22.columns)
imputed_df_22

imputed_df_22.info()

Y = imputed_df_22.overall
X = imputed_df_22.drop(columns='overall')

X_scaled = sc.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)
X

"""**Use model to predict with new data and check accuracy**"""

# Model on new players_22 data
players_22_prediction = model_2.predict(X)
mean_absolute_error(Y, players_22_prediction)

"""## Save Model

**Saving Model and Scaler object for deployment purposes**
"""

import pickle

"""Both the scaler and model objects are saved in files. Scaler objects are used to ensure that the input values of users in the deployed site are scaled using the same standard deviation values as the one with which the model was trained with."""

# Scaler object as file to scale user input with the necessary parameters
pickle.dump(sc, open('/content/drive/MyDrive/Colab Notebooks/new_scaler_parameters.pkl', 'wb'))

# Model object as file to predict the scaled values after user input
pickle.dump(model_2, open('/content/drive/MyDrive/Colab Notebooks/new_player_ratings_model.pkl', 'wb'))

